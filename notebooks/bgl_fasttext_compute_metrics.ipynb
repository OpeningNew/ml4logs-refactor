{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_curve,\n",
    "                             roc_auc_score,\n",
    "                             average_precision_score,\n",
    "                             precision_recall_fscore_support)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path(os.getenv('PROJECT_DIR', default=pathlib.Path.home() / 'ml4logs'))\n",
    "DATASET_PATH = PROJECT_DIR / 'data/processed/bgl.npz'\n",
    "REPORT_DIR = PROJECT_DIR / \"reports/results\"\n",
    "\n",
    "assert(DATASET_PATH.exists() and DATASET_PATH.is_file())\n",
    "assert(REPORT_DIR.exists() and REPORT_DIR.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(DATASET_PATH)"
   ]
  },
  {
   "source": [
    "## END TODO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Tree': DecisionTreeClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(C=100, tol=1e-2, max_iter=10**3),\n",
    "    'LinearSVC': LinearSVC(penalty='l1', tol=0.1, dual=False),\n",
    "    'IsolationForest': IsolationForest(random_state=2019, max_samples=0.9999, contamination=0.03, n_jobs=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = []\n",
    "score = []\n",
    "for name, model in models.items():\n",
    "    display(name)\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "    display(f'{name} is fitted')\n",
    "    c_pred = model.predict(x_test_scaled)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred = model.predict_proba(x_test_scaled)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred = model.decision_function(x_test_scaled)\n",
    "        if (name in {\"IsolationForest\", \"OneClassSVM\"}):\n",
    "            y_pred = -y_pred\n",
    "            c_pred[c_pred == 1] = 0\n",
    "            c_pred[c_pred == -1] = 1\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_pred)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, c_pred, average='binary', zero_division=0)\n",
    "\n",
    "    stats = {\n",
    "        'Model': name,\n",
    "        'AUC': auc,\n",
    "        'AP': ap,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "    roc.append(pd.DataFrame({'Model': name, 'FPR': fpr, 'TPR': tpr}))\n",
    "    score.append(pd.DataFrame([stats]))\n",
    "    display(f'{name} is evaluated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(roc).to_csv(REPORT_DIR / 'bgl-fasttext-loglizer-roc.csv', index=False)\n",
    "pd.concat(score).to_csv(REPORT_DIR / 'bgl-fasttext-loglizer-score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}